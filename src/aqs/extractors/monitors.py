"""Extractors for AQS monitor data.

This module contains the existing metadata extractor (`fetch_monitors`) plus
additional, sample-first helpers to fetch AQS `sampleData/bySite` results.

Design notes:
- The repository prefers lazy registration of AQS credentials via
  `src/soar/config.set_aqs_credentials()`. Sample/live HTTP helpers below will
  read `config.AQS_EMAIL` and `config.AQS_KEY` when building API URLs.
- The sample-data helpers use the same per-year splitting logic as the R
  reference implementation (one request per calendar year chunk).
"""

from __future__ import annotations

from datetime import date
from typing import Iterable, Optional, List

from urllib.parse import urlencode
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
import requests

from pyaqsapi import bystate
import config
from aqs import _client


def _add_site_code(df: pd.DataFrame) -> pd.DataFrame:
    """Add site_code column to monitor DataFrame.

    Creates site_code as: state_code (2 digits) + county_code (3 digits) + site_number (4 digits)
    For Oregon data, defaults state_code to 41 if missing/invalid.
    """
    df = df.copy()

    # Handle NaN and non-numeric values robustly
    df["state_code_num"] = pd.to_numeric(df["state_code"], errors="coerce")
    df["county_code_num"] = pd.to_numeric(df["county_code"], errors="coerce")
    df["site_number_num"] = pd.to_numeric(df["site_number"], errors="coerce")

    # Default to Oregon state code (41) if missing or invalid
    df["state_code_num"] = df["state_code_num"].fillna(41).astype(int)
    df["state_code_num"] = df["state_code_num"].where(df["state_code_num"] == 41, 41)

    # Fill missing county/site codes with 0
    df["county_code_num"] = df["county_code_num"].fillna(0).astype(int)
    df["site_number_num"] = df["site_number_num"].fillna(0).astype(int)

    df["site_code"] = (
        df["state_code_num"].astype(str).str.zfill(2)
        + df["county_code_num"].astype(str).str.zfill(3)
        + df["site_number_num"].astype(str).str.zfill(4)
    )

    # Clean up temporary columns
    df = df.drop(columns=["state_code_num", "county_code_num", "site_number_num"])

    return df


def _ensure_dataframe(payload: object) -> Optional[pd.DataFrame]:
    """Normalize pyaqsapi responses into a pandas DataFrame."""
    if payload is None:
        return None
    if isinstance(payload, pd.DataFrame):
        return payload if not payload.empty else None
    if hasattr(payload, "get_data"):
        return _ensure_dataframe(payload.get_data())
    if isinstance(payload, list):
        frames = [
            frame
            for frame in (_ensure_dataframe(item) for item in payload)
            if frame is not None and not frame.empty
        ]
        if not frames:
            return None
        return pd.concat(frames, ignore_index=True)
    try:
        frame = pd.DataFrame(payload)
    except ValueError:  # pragma: no cover - fall back when payload is not coercible
        return None
    return frame if not frame.empty else None


def fetch_monitors(
    parameter_codes: Iterable[str], bdate: date, edate: date, state_fips: str
) -> pd.DataFrame:
    """Fetch monitor metadata for the provided parameters and time window.

    This is the existing metadata fetcher (keeps using `pyaqsapi.bystate.monitors`).
    """
    frames: list[pd.DataFrame] = []
    for code in parameter_codes:
        response = bystate.monitors(code, bdate, edate, state_fips, return_header=True)
        frame = _ensure_dataframe(response)
        if frame is None:
            continue
        enriched = frame.copy()
        enriched["seed_parameter_code"] = code
        frames.append(enriched)
    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)


def fetch_aqs_response(api_url: str) -> pd.DataFrame:
    """Fetch a raw AQS API URL and return a DataFrame of the data payload.

    The AQS JSON responses are structured as a two-element result where the
    second element is the data array (the R script used `[[2]]`). We mirror that
    behavior here.
    """
    resp = requests.get(api_url)
    resp.raise_for_status()
    parsed = resp.json()
    # AQS typically returns [header, data]; if data missing return empty frame
    data = parsed[1] if isinstance(parsed, list) and len(parsed) > 1 else []
    if not data:
        return pd.DataFrame()
    return pd.DataFrame(data)


def build_aqs_requests(
    state_code: str,
    county_code: str,
    site_number: str,
    parameter_code: str,
    start_date: date | str,
    end_date: date | str,
) -> List[str]:
    """Return a list of AQS sampleData/bySite request URLs split by calendar year.

    start_date and end_date may be datetime.date or ISO strings. Returned
    URLs include the configured `config.AQS_EMAIL` and `config.AQS_KEY`.
    """
    # Normalize dates
    sdate = pd.to_datetime(start_date).date()
    edate = pd.to_datetime(end_date).date()
    years = range(sdate.year, edate.year + 1)

    urls: List[str] = []
    for i, y in enumerate(years):
        if i == 0:
            b = sdate.strftime("%Y%m%d")
        else:
            b = f"{y}0101"
        if i == len(years) - 1:
            e = edate.strftime("%Y%m%d")
        else:
            e = f"{y}1231"

        params = {
            "email": config.AQS_EMAIL or "",
            "key": config.AQS_KEY or "",
            "param": parameter_code,
            "bdate": b,
            "edate": e,
            "state": state_code,
            "county": county_code,
            "site": site_number,
        }
        url = f"https://aqs.epa.gov/data/api/sampleData/bySite?{urlencode(params)}"
        urls.append(url)

    return urls


def fetch_samples_for_parameter(
    parameter_code: str, bdate: date, edate: date, state_fips: str
) -> pd.DataFrame:
    """Fetch sample-level data for all sites that report the given parameter.

    Flow:
    1. Use `fetch_monitors` to obtain monitor metadata (sites) for the parameter.
    2. For each unique (state, county, site) build per-year URLs and fetch sample data.
    3. Concatenate results and add a `parameter` column with the provided code.

    This function is sample-first and uses the AQS REST endpoint (same as the
    R example). It is safe to call without credentials in tests because the HTTP
    calls are mockable. For large parameter lists consider streaming writes to
    disk instead of keeping all data in memory.
    """
    monitors = fetch_monitors([parameter_code], bdate, edate, state_fips)
    if monitors.empty:
        return pd.DataFrame()

    samples_frames: List[pd.DataFrame] = []
    # Expect monitors to contain state_code, county_code, site_number columns
    unique_sites = monitors[
        ["state_code", "county_code", "site_number", "parameter_code"]
    ].drop_duplicates()
    for _, row in unique_sites.iterrows():
        state = str(row["state_code"]).zfill(2)
        county = str(row["county_code"]).zfill(3)
        site = str(row["site_number"]).zfill(3)
        urls = build_aqs_requests(state, county, site, parameter_code, bdate, edate)
        site_frames: List[pd.DataFrame] = []
        for url in urls:
            df = fetch_aqs_response(url)
            if df.empty:
                continue
            site_frames.append(df)
        if not site_frames:
            continue
        site_data = pd.concat(site_frames, ignore_index=True)
        site_data["parameter"] = parameter_code
        # keep a lightweight site id column similar to other transforms
        site_data["site_number"] = site
        samples_frames.append(site_data)

    if not samples_frames:
        return pd.DataFrame()


def fetch_all_monitors_for_oregon(bdate: date, edate: date) -> pd.DataFrame:
    """Fetch all unique monitor metadata for Oregon (state 41) from 2005-2025.

    Iterates through each parameter to ensure complete dataset extraction,
    fetches monitors for each parameter across the full date range, concatenates results,
    and deduplicates by site_code to ensure one entry per unique monitor location,
    regardless of parameter coverage.
    """
    # Check circuit breaker
    if _client.circuit_is_open():
        raise RuntimeError("AQS circuit is open; cannot fetch monitors")

    # Hardcoded parameter codes
    parameter_codes = [
        "88101",
        "88502",
        "81102",
        "44201",
        "42101",
        "42602",
        "14129",
        "85129",
        "61101",
        "82128",
        "17141",
        "45201",
        "14115",
        "43502",
        "58103",
        "85102",
    ]

    print(
        f"üìã Processing {len(parameter_codes)} parameters for monitors from {bdate} to {edate}..."
    )

    # Fetch monitors for each parameter across the full date range (more efficient)
    all_monitors: List[pd.DataFrame] = []

    def fetch_for_param(code: str) -> pd.DataFrame:
        print(f"  üì° Fetching monitors for parameter {code}...")
        monitors = fetch_monitors([code], bdate, edate, "41")  # Oregon FIPS
        if not monitors.empty:
            print(f"  ‚úÖ Found {len(monitors)} monitors for {code}")
        else:
            print(f"  ‚ö†Ô∏è  No monitors found for {code}")
        return monitors

    # Use ThreadPoolExecutor for concurrent fetching
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(fetch_for_param, code) for code in parameter_codes]
        for future in futures:
            df = future.result()
            if not df.empty:
                all_monitors.append(df)

    # Concatenate and deduplicate by site_code (one entry per monitor location)
    if not all_monitors:
        print("‚ùå No monitors found for any parameter")
        return pd.DataFrame()

    print("üîÑ Concatenating and deduplicating monitor data...")
    combined = pd.concat(all_monitors, ignore_index=True)
    original_count = len(combined)

    # Create site_code efficiently
    combined = _add_site_code(combined)

    # Deduplicate by site_code (one entry per monitor location)
    combined = combined.drop_duplicates(subset=["site_code"])
    deduped_count = len(combined)

    print(
        f"‚úÖ Deduplicated: {original_count} raw entries ‚Üí {deduped_count} unique monitor locations (by site_code)"
    )
    return combined
